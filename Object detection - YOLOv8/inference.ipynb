{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16909c89",
   "metadata": {},
   "source": [
    "Code for inference on a trained YOLOv8n model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8n model from .pt file\n",
    "model = YOLO(r\"D:\\runs\\detect\\train\\weights\\best.pt\")\n",
    "\n",
    "# Load image for inference\n",
    "image_path = r\"D:\\test\\images\\filename.jpg\"\n",
    "\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41291992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "results = model(image)\n",
    "\n",
    "# Visualize results\n",
    "annotated_frame = results[0].plot()  # Draws boxes and labels on the image\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"Detection\", annotated_frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Optionally, save the annotated image\n",
    "cv2.imwrite(\"img_pred.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val(\n",
    "    data=r\"D:\\data.yaml\",  # change path in yaml file\n",
    "    conf=0.25,                 # confidence threshold\n",
    "    iou=0.5,                   # IoU threshold for matching\n",
    "    split=\"test\",              # 'valid' or 'test' split\n",
    "    plots=True                # Save confusion matrix and other plots\n",
    ")\n",
    "\n",
    "# Access confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix.matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c360bde2",
   "metadata": {},
   "source": [
    "📊 YOLOv8 Evaluation Metrics Class\n",
    "\n",
    "This class is designed for computing and storing evaluation metrics for YOLOv8 models.\n",
    "\n",
    "    metrics.save_dir\n",
    "\n",
    "    Attributes:\n",
    "        save_dir (Path): A path to the directory where the output plots will be saved.\n",
    "        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n",
    "        names (dict): A dictionary of class names.\n",
    "        box (Metric): An instance of the Metric class for storing detection results.\n",
    "        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n",
    "        task (str): The task type, set to 'detect'.\n",
    "\n",
    "\n",
    "\n",
    "    metrics.box\n",
    "\n",
    "🧬 Attributes\n",
    "\n",
    "| Attribute        | Type | Description                                                        |\n",
    "| ---------------- | ---- | ------------------------------------------------------------------ |\n",
    "| `p`              | list | Precision for each class. Shape: `(nc,)`                           |\n",
    "| `r`              | list | Recall for each class. Shape: `(nc,)`                              |\n",
    "| `f1`             | list | F1 score for each class. Shape: `(nc,)`                            |\n",
    "| `all_ap`         | list | AP scores for all classes across IoU thresholds. Shape: `(nc, 10)` |\n",
    "| `ap_class_index` | list | Index of class for each AP score. Shape: `(nc,)`                   |\n",
    "| `nc`             | int  | Number of classes                                                  |\n",
    "\n",
    "⚙️ Methods\n",
    "\n",
    "| Method            | Returns | Description                                                         |\n",
    "| ----------------- | ------- | ------------------------------------------------------------------- |\n",
    "| `ap50()`          | `list`  | AP at IoU = 0.5 for all classes. Shape: `(nc,)` or `[]`             |\n",
    "| `ap()`            | `list`  | AP at IoUs from 0.5 to 0.95 for all classes. Shape: `(nc,)` or `[]` |\n",
    "| `mp()`            | `float` | Mean precision across all classes                                   |\n",
    "| `mr()`            | `float` | Mean recall across all classes                                      |\n",
    "| `map50()`         | `float` | Mean AP at IoU = 0.5 across all classes                             |\n",
    "| `map75()`         | `float` | Mean AP at IoU = 0.75 across all classes                            |\n",
    "| `map()`           | `float` | Mean AP from IoU 0.5 to 0.95 across all classes                     |\n",
    "| `mean_results()`  | `tuple` | Returns tuple: `(mp, mr, map50, map)`                               |\n",
    "| `class_result(i)` | `tuple` | Returns class-specific metrics: `(p[i], r[i], ap50[i], ap[i])`      |\n",
    "| `maps()`          | `list`  | Returns mAP scores for each class. Shape: `(nc,)`                   |\n",
    "| `fitness()`       | `float` | Returns model fitness score based on weighted metrics               |\n",
    "| `update(results)` | `None`  | Updates internal metrics with new evaluation results                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f52d5d",
   "metadata": {},
   "source": [
    "Calculate Precision, Recall, mAP50, mAP50-95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = metrics.results_dict\n",
    "# print(results_dict)\n",
    "results_keys = metrics.results_dict.keys()\n",
    "print(results_keys, '\\n')\n",
    "\n",
    "precision = results_dict['metrics/precision(B)']*100\n",
    "recall = results_dict['metrics/recall(B)']*100\n",
    "mAP50 = results_dict['metrics/mAP50(B)']*100\n",
    "mAP50_95 = results_dict['metrics/mAP50-95(B)']*100\n",
    "\n",
    "print(f'Precision: {precision:.1f}')\n",
    "print(f'Recall: {recall:.1f}')\n",
    "print(f'mAP50: {mAP50:.1f}')\n",
    "print(f'mAP50-95: {mAP50_95:.1f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f0f2b",
   "metadata": {},
   "source": [
    "Plot Confusion Matrix Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "font_size = 12\n",
    "fig_size = 6\n",
    "\n",
    "cm = metrics.confusion_matrix\n",
    "\n",
    "# Plot confusion matrix manually\n",
    "fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "im = ax.imshow(cm.matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.colorbar(im)\n",
    "\n",
    "# change class according to dataset\n",
    "classes = ['bolt', 'tie-plate', 'missing bolt', 'clip', 'missing clip', 'Defect FP', 'Non-defect FP', 'background']\n",
    "\n",
    "ax.set(xticks=range(len(classes)),\n",
    "       yticks=range(len(classes)),\n",
    "       xticklabels=classes, \n",
    "       yticklabels=classes,\n",
    "       )\n",
    "\n",
    "ax.set_xlabel('True', fontsize=font_size, fontweight='bold')\n",
    "ax.set_ylabel('Predicted', fontsize=font_size, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix', fontsize=font_size, fontweight='bold')\n",
    "\n",
    "# Rotate tick labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=font_size)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=font_size)\n",
    "\n",
    "# Annotate cells with values\n",
    "fmt = 'd'\n",
    "thresh = cm.matrix.max() / 2.\n",
    "for i in range(cm.matrix.shape[0]):\n",
    "    for j in range(cm.matrix.shape[1]):\n",
    "        if int(cm.matrix[i, j]) > 0:\n",
    "            ax.text(j, i, format(int(cm.matrix[i, j]), fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm.matrix[i, j] > thresh else \"black\",\n",
    "                    fontsize=font_size)  # <-- change this font size\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"custom_confusion_matrix.png\", dpi=500)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a0b7c",
   "metadata": {},
   "source": [
    "Plot normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c58566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the confusion matrix row-wise\n",
    "matrix = cm.matrix.astype('float')\n",
    "normalized_matrix = matrix / matrix.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "im = ax.imshow(normalized_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set(\n",
    "    xticks=range(len(classes)),\n",
    "    yticks=range(len(classes)),\n",
    "    xticklabels=classes,\n",
    "    yticklabels=classes\n",
    ")\n",
    "ax.set_xlabel('True', fontsize=font_size, fontweight='bold')\n",
    "ax.set_ylabel('Predicted', fontsize=font_size, fontweight='bold')\n",
    "ax.set_title('Normalized Confusion Matrix', fontsize=font_size, fontweight='bold')\n",
    "\n",
    "# Set tick label font sizes\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=font_size)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=font_size)\n",
    "\n",
    "# Annotate each cell only if value > 0\n",
    "fmt = '.2f'\n",
    "thresh = normalized_matrix.max() / 2.\n",
    "for i in range(normalized_matrix.shape[0]):\n",
    "    for j in range(normalized_matrix.shape[1]):\n",
    "        val = normalized_matrix[i, j]\n",
    "        if val > 0.01:\n",
    "            ax.text(j, i, format(val, fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if val > thresh else \"black\",\n",
    "                    fontsize=font_size)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"normalized_confusion_matrix.png\", dpi=500)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
